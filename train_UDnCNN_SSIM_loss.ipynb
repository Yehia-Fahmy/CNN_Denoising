{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9KVxNq313dP"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dcyQ4LdzZ67"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Sequential, Model\n",
        "import json\n",
        "from datetime import datetime\n",
        "from keras.layers import Conv2D, BatchNormalization, ReLU, Add, Input, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enable GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#     try:\n",
        "#         for gpu in gpus:\n",
        "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#         tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "#         print(\"Using GPU:\", gpus[0])\n",
        "#     except RuntimeError as e:\n",
        "#         print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrk-gzKi3Bao"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define image paths\n",
        "clean_path = \"/home/yehia/celaba_datasets/celeba_128x128_10000/clean\"\n",
        "noisy_path = \"/home/yehia/celaba_datasets/celeba_128x128_10000/noisy\"\n",
        "\n",
        "# Load all image filenames\n",
        "image_filenames = sorted(os.listdir(clean_path))\n",
        "\n",
        "# Load images\n",
        "clean_images = []\n",
        "noisy_images = []\n",
        "\n",
        "for fname in image_filenames:\n",
        "    clean_img = Image.open(os.path.join(clean_path, fname)).convert('RGB')\n",
        "    noisy_img = Image.open(os.path.join(noisy_path, fname)).convert('RGB')\n",
        "    \n",
        "    clean_images.append(np.array(clean_img))\n",
        "    noisy_images.append(np.array(noisy_img))\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "clean_images = np.array(clean_images)\n",
        "noisy_images = np.array(noisy_images)\n",
        "\n",
        "# Split into training and testing sets\n",
        "x_train_clean, x_test_clean, x_train_noisy, x_test_noisy = train_test_split(\n",
        "    clean_images, noisy_images, test_size=0.1, random_state=42)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train_clean = x_train_clean.astype('float32') / 255.\n",
        "x_test_clean = x_test_clean.astype('float32') / 255.\n",
        "x_train_noisy = x_train_noisy.astype('float32') / 255.\n",
        "x_test_noisy = x_test_noisy.astype('float32') / 255.\n",
        "\n",
        "# Shuffle data while maintaining alignment\n",
        "perm = np.random.permutation(len(x_train_clean))\n",
        "x_train_clean = x_train_clean[perm]\n",
        "x_train_noisy = x_train_noisy[perm]\n",
        "\n",
        "perm_test = np.random.permutation(len(x_test_clean))\n",
        "x_test_clean = x_test_clean[perm_test]\n",
        "x_test_noisy = x_test_noisy[perm_test]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UuBLkW25p3-"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JOiKQEwy5sKJ",
        "outputId": "c1ac5560-708e-461e-963a-d1654460f171"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of training images: {x_train_clean.shape[0]}\")\n",
        "print(f\"Number of test images: {x_test_clean.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distribution and Value Ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Image shape (train):\", x_train_clean[0].shape)\n",
        "print(\"Pixel value range (clean):\", x_train_clean.min(), \"to\", x_train_clean.max())\n",
        "print(\"Pixel value range (noisy):\", x_train_noisy.min(), \"to\", x_train_noisy.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pixel Intensity Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(x_train_clean.flatten(), bins=50, color='blue', alpha=0.7)\n",
        "plt.title(\"Clean Image Pixel Distribution\")\n",
        "plt.xlabel(\"Pixel intensity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(x_train_noisy.flatten(), bins=50, color='red', alpha=0.7)\n",
        "plt.title(\"Noisy Image Pixel Distribution\")\n",
        "plt.xlabel(\"Pixel intensity\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_samples(clean, noisy, num_samples=5):\n",
        "    plt.figure(figsize=(num_samples * 2, 4))\n",
        "    for i in range(num_samples):\n",
        "        # Noisy\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(noisy[i])\n",
        "        plt.title(\"Noisy\")\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Clean\n",
        "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
        "        plt.imshow(clean[i])\n",
        "        plt.title(\"Clean\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_samples(x_train_clean, x_train_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGiURME3-qH"
      },
      "source": [
        "## Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "batch_size = 16\n",
        "alpha = 0.3\n",
        "loss_function = \"ssim_mae\"\n",
        "model_name = f\"UDnCNN_{loss_function}_a{alpha}_e{epochs}_b{batch_size}_v1\"\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SSIM Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combined_loss(y_true, y_pred, alpha=alpha):\n",
        "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
        "    ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
        "    return alpha * mae + (1 - alpha) * (1 - ssim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4XiApan38tv",
        "outputId": "9ca2f68d-b55c-4d84-a8b6-d68e2df11b7f"
      },
      "outputs": [],
      "source": [
        "input_img = Input(shape=(x_train_clean.shape[1], x_train_clean.shape[2], x_train_clean.shape[3]))\n",
        "\n",
        "# Initial Conv + ReLU\n",
        "x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
        "x = ReLU()(x)\n",
        "\n",
        "# Downsampling block 1\n",
        "x1 = Conv2D(64, (3, 3), padding='same')(x)\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = ReLU()(x1)\n",
        "p1 = MaxPooling2D(pool_size=(2, 2))(x1)\n",
        "\n",
        "# Downsampling block 2\n",
        "x2 = Conv2D(64, (3, 3), padding='same')(p1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 = ReLU()(x2)\n",
        "p2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
        "\n",
        "# Middle block 1\n",
        "x3 = Conv2D(64, (3, 3), padding='same')(p2)\n",
        "x3 = BatchNormalization()(x3)\n",
        "x3 = ReLU()(x3)\n",
        "\n",
        "# Middle block 2\n",
        "x4 = Conv2D(64, (3, 3), padding='same')(x3)\n",
        "x4 = BatchNormalization()(x4)\n",
        "x4 = ReLU()(x4)\n",
        "\n",
        "# Upsampling block 1 + skip connection with x2\n",
        "u1 = UpSampling2D(size=(2, 2))(x4)\n",
        "u1 = Add()([u1, x2])\n",
        "u1 = Conv2D(64, (3, 3), padding='same')(u1)\n",
        "u1 = BatchNormalization()(u1)\n",
        "u1 = ReLU()(u1)\n",
        "\n",
        "# Upsampling block 2 + skip connection with x1\n",
        "u2 = UpSampling2D(size=(2, 2))(u1)\n",
        "u2 = Add()([u2, x1])\n",
        "u2 = Conv2D(64, (3, 3), padding='same')(u2)\n",
        "u2 = BatchNormalization()(u2)\n",
        "u2 = ReLU()(u2)\n",
        "\n",
        "# Final output layer\n",
        "output = Conv2D(3, (3, 3), padding='same')(u2)\n",
        "\n",
        "# Skip Connection (Residual Learning)\n",
        "output_img = Add()([input_img, output])\n",
        "\n",
        "model = Model(inputs=input_img, outputs=output_img)\n",
        "\n",
        "model.compile(optimizer='adam', loss=combined_loss)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Callback to Track Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeHistory(Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.start_time = time.time()  # returns float in seconds\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.total_time = time.time() - self.start_time\n",
        "\n",
        "time_callback = TimeHistory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNlBUXuX5YCo",
        "outputId": "e322b4db-d9bd-4165-b1c7-756efa3bf3a2"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x_train_noisy, x_train_clean,\n",
        "    validation_data=(x_test_noisy, x_test_clean),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[time_callback]\n",
        ")\n",
        "print(\"Training time (seconds):\", round(time_callback.total_time, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dir = f\"./models/{model_name}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "model.save(os.path.join(model_dir, \"model.keras\"))\n",
        "print(f\"\\n✅ Model saved to: {model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "mae = np.mean(np.abs(predictions - x_test_clean))\n",
        "mse = np.mean((predictions - x_test_clean) ** 2)\n",
        "\n",
        "psnr_total, ssim_total = 0, 0\n",
        "for i in range(len(x_test_clean)):\n",
        "    psnr_total += peak_signal_noise_ratio(x_test_clean[i], predictions[i], data_range=1.0)\n",
        "    ssim_total += structural_similarity(x_test_clean[i], predictions[i], channel_axis=-1, data_range=1.0)\n",
        "\n",
        "psnr_avg = psnr_total / len(x_test_clean)\n",
        "ssim_avg = ssim_total / len(x_test_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata = {\n",
        "    \"model_name\": model_name,\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"loss_function\": loss_function,\n",
        "    \"alpha\": alpha,\n",
        "    \"epochs\": int(epochs),\n",
        "    \"batch_size\": int(batch_size),\n",
        "    \"training_time_seconds\": round(float(time_callback.total_time), 2),\n",
        "    \"dataset_info\": {\n",
        "        \"train_size\": int(x_train_clean.shape[0]),\n",
        "        \"test_size\": int(x_test_clean.shape[0]),\n",
        "        \"image_shape\": list(map(int, x_train_clean.shape[1:]))\n",
        "    },\n",
        "    \"evaluation_metrics\": {\n",
        "        \"mae\": round(float(mae), 6),\n",
        "        \"mse\": round(float(mse), 6),\n",
        "        \"psnr_avg\": round(float(psnr_avg), 3),\n",
        "        \"ssim_avg\": round(float(ssim_avg), 3)\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(model_dir, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(f\"\\n✅ Metadata saved to: {model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz1_i-Qx696G"
      },
      "source": [
        "### Visualize the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_predictions(noisy, clean, predicted, num_images=5):\n",
        "    plt.figure(figsize=(num_images * 3, 9))\n",
        "    for i in range(num_images):\n",
        "        # Noisy\n",
        "        plt.subplot(3, num_images, i + 1)\n",
        "        plt.imshow(noisy[i])\n",
        "        plt.title(\"Noisy\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predicted\n",
        "        plt.subplot(3, num_images, i + 1 + num_images)\n",
        "        plt.imshow(predicted[i])\n",
        "        plt.title(\"Denoised\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Clean\n",
        "        plt.subplot(3, num_images, i + 1 + num_images * 2)\n",
        "        plt.imshow(clean[i])\n",
        "        plt.title(\"Clean\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_predictions(x_test_noisy, x_test_clean, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_and_save_denoising(noisy, predicted, clean, model_dir, num_images=5):\n",
        "    \"\"\"\n",
        "    Visualize and save a comparison of noisy, denoised, and clean images.\n",
        "    \n",
        "    Args:\n",
        "        noisy (numpy array): Noisy input images\n",
        "        predicted (numpy array): Model predictions (denoised images)\n",
        "        clean (numpy array): Ground truth clean images\n",
        "        model_dir (str): Directory where the visualization image will be saved\n",
        "        num_images (int): Number of image samples to visualize\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(num_images * 3, 9))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Noisy input\n",
        "        plt.subplot(3, num_images, i + 1)\n",
        "        plt.imshow(noisy[i])\n",
        "        plt.title(\"Noisy\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Denoised (model output)\n",
        "        plt.subplot(3, num_images, i + 1 + num_images)\n",
        "        plt.imshow(predicted[i])\n",
        "        plt.title(\"Denoised\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Ground truth\n",
        "        plt.subplot(3, num_images, i + 1 + 2 * num_images)\n",
        "        plt.imshow(clean[i])\n",
        "        plt.title(\"Clean\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save figure\n",
        "    save_path = os.path.join(model_dir, \"denoising_visualization.png\")\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()  # Close the figure to free memory\n",
        "\n",
        "    print(f\"📸 Visualization saved to: {save_path}\")\n",
        "\n",
        "visualize_and_save_denoising(\n",
        "    noisy=x_test_noisy,\n",
        "    predicted=predictions,\n",
        "    clean=x_test_clean,\n",
        "    model_dir=model_dir,\n",
        "    num_images=5  # You can increase this number if desired\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep CNN Autoencoder - Denoising Image.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf310_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
