{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9KVxNq313dP"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dcyQ4LdzZ67"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Sequential, Model\n",
        "import json\n",
        "from datetime import datetime\n",
        "from keras.layers import Conv2D, BatchNormalization, ReLU, Add, Input, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enable GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#     try:\n",
        "#         for gpu in gpus:\n",
        "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#         tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "#         print(\"Using GPU:\", gpus[0])\n",
        "#     except RuntimeError as e:\n",
        "#         print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrk-gzKi3Bao"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define image paths\n",
        "clean_path = \"/home/yehia/celaba_datasets/celeba_64x64_10000/clean\"\n",
        "noisy_path = \"/home/yehia/celaba_datasets/celeba_64x64_10000/noisy\"\n",
        "\n",
        "# Load all image filenames\n",
        "image_filenames = sorted(os.listdir(clean_path))\n",
        "\n",
        "# Load images\n",
        "clean_images = []\n",
        "noisy_images = []\n",
        "\n",
        "for fname in image_filenames:\n",
        "    clean_img = Image.open(os.path.join(clean_path, fname)).convert('RGB')\n",
        "    noisy_img = Image.open(os.path.join(noisy_path, fname)).convert('RGB')\n",
        "    \n",
        "    clean_images.append(np.array(clean_img))\n",
        "    noisy_images.append(np.array(noisy_img))\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "clean_images = np.array(clean_images)\n",
        "noisy_images = np.array(noisy_images)\n",
        "\n",
        "# Split into training and testing sets\n",
        "x_train_clean, x_test_clean, x_train_noisy, x_test_noisy = train_test_split(\n",
        "    clean_images, noisy_images, test_size=0.1, random_state=42)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train_clean = x_train_clean.astype('float32') / 255.\n",
        "x_test_clean = x_test_clean.astype('float32') / 255.\n",
        "x_train_noisy = x_train_noisy.astype('float32') / 255.\n",
        "x_test_noisy = x_test_noisy.astype('float32') / 255.\n",
        "\n",
        "# Shuffle data while maintaining alignment\n",
        "perm = np.random.permutation(len(x_train_clean))\n",
        "x_train_clean = x_train_clean[perm]\n",
        "x_train_noisy = x_train_noisy[perm]\n",
        "\n",
        "perm_test = np.random.permutation(len(x_test_clean))\n",
        "x_test_clean = x_test_clean[perm_test]\n",
        "x_test_noisy = x_test_noisy[perm_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_samples(clean, noisy, label1=\"Clean\", label2=\"Noisy\", num_samples=5):\n",
        "    plt.figure(figsize=(num_samples * 2, 4))\n",
        "    for i in range(num_samples):\n",
        "        # Noisy\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(noisy[i])\n",
        "        plt.title(label2)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Clean\n",
        "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
        "        plt.imshow(clean[i])\n",
        "        plt.title(label1)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def show_samples_fourier(clean, noisy, label1=\"Clean\", label2=\"Noisy\", num_samples=5):\n",
        "    plt.figure(figsize=(num_samples * 2, 4))\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # Handle noisy\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        f_noisy = noisy[i]\n",
        "        if isinstance(f_noisy, list):  # color image (list of 3 fshifts)\n",
        "            mags = [20 * np.log(np.abs(ch) + 1) for ch in f_noisy]\n",
        "            combined = np.mean(mags, axis=0)  # average magnitude over RGB\n",
        "        else:  # grayscale\n",
        "            combined = 20 * np.log(np.abs(f_noisy) + 1)\n",
        "        plt.imshow(combined, cmap='gray')\n",
        "        plt.title(label2)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Handle clean\n",
        "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
        "        f_clean = clean[i]\n",
        "        if isinstance(f_clean, list):  # color\n",
        "            mags = [20 * np.log(np.abs(ch) + 1) for ch in f_clean]\n",
        "            combined = np.mean(mags, axis=0)\n",
        "        else:  # grayscale\n",
        "            combined = 20 * np.log(np.abs(f_clean) + 1)\n",
        "        plt.imshow(combined, cmap='gray')\n",
        "        plt.title(label1)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fourier_transform_image(img):\n",
        "    if len(img.shape) == 2:\n",
        "        fshift = np.fft.fftshift(np.fft.fft2(img))\n",
        "        return fshift\n",
        "    else:\n",
        "        channels = cv2.split(img)\n",
        "        fshifts = [np.fft.fftshift(np.fft.fft2(ch)) for ch in channels]\n",
        "        return fshifts\n",
        "\n",
        "def inverse_fourier_transform(fshifts):\n",
        "    if isinstance(fshifts, np.ndarray):\n",
        "        f_ishift = np.fft.ifftshift(fshifts)\n",
        "        img_back = np.fft.ifft2(f_ishift)\n",
        "        img_back = np.abs(img_back)\n",
        "        return cv2.normalize(img_back, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    else:\n",
        "        channels = []\n",
        "        for fshift in fshifts:\n",
        "            f_ishift = np.fft.ifftshift(fshift)\n",
        "            img_back = np.fft.ifft2(f_ishift)\n",
        "            img_back = np.abs(img_back)\n",
        "            img_back = cv2.normalize(img_back, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "            channels.append(img_back)\n",
        "        return cv2.merge(channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Fourier Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process Entire Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_clean_fourier = [fourier_transform_image(img) for img in x_train_clean]\n",
        "x_train_noisy_fourier = [fourier_transform_image(img) for img in x_train_noisy]\n",
        "x_test_clean_fourier = [fourier_transform_image(img) for img in x_test_clean]\n",
        "x_test_noisy_fourier = [fourier_transform_image(img) for img in x_test_noisy]\n",
        "\n",
        "show_samples(x_train_clean, x_train_noisy)\n",
        "show_samples_fourier(x_train_clean_fourier, x_train_noisy_fourier, \"Clean Fourier\", \"Noisy Fourier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Comlex Channels for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_complex_channels(data):\n",
        "    split_data = []\n",
        "    for img_fft in data:  # img_fft: list of 3 channels (f_r, f_g, f_b)\n",
        "        real_parts = [np.real(ch) for ch in img_fft]\n",
        "        imag_parts = [np.imag(ch) for ch in img_fft]\n",
        "        stacked = np.stack(real_parts + imag_parts, axis=-1)  # shape: (H, W, 6)\n",
        "        split_data.append(stacked)\n",
        "    return np.array(split_data, dtype=np.float32)  # shape: (N, H, W, 6)\n",
        "\n",
        "def combine_complex_channels(data):\n",
        "    assert data.shape[-1] == 6, \"Input must have 6 channels (3 real + 3 imag)\"\n",
        "    real = data[..., :3]\n",
        "    imag = data[..., 3:]\n",
        "    combined = real + 1j * imag  # shape: (N, H, W, 3), complex\n",
        "    return [ [combined[i, ..., ch] for ch in range(3)] for i in range(data.shape[0])]\n",
        "\n",
        "def normalize_globally(data):\n",
        "    mean = np.mean(data, axis=(0, 1, 2), keepdims=True)  # (1, 1, 1, 6)\n",
        "    std = np.std(data, axis=(0, 1, 2), keepdims=True)\n",
        "    return (data - mean) / (std + 1e-8), mean, std\n",
        "\n",
        "def denormalize_globally(data, mean, std):\n",
        "    return data * (std + 1e-8) + mean\n",
        "\n",
        "x_train_clean_fourier_split = split_complex_channels(x_train_clean_fourier)\n",
        "x_train_noisy_fourier_split = split_complex_channels(x_train_noisy_fourier)\n",
        "x_test_clean_fourier_split = split_complex_channels(x_test_clean_fourier)\n",
        "x_test_noisy_fourier_split = split_complex_channels(x_test_noisy_fourier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Concatenate all data to compute a shared global mean/std\n",
        "all_split = np.concatenate([\n",
        "    x_train_clean_fourier_split,\n",
        "    x_train_noisy_fourier_split,\n",
        "    x_test_clean_fourier_split,\n",
        "    x_test_noisy_fourier_split\n",
        "], axis=0)\n",
        "\n",
        "# Step 2: Compute global mean and std\n",
        "all_normalized, global_mean, global_std = normalize_globally(all_split)\n",
        "\n",
        "# Step 3: Split normalized data back into the original sets\n",
        "n_train_clean = len(x_train_clean_fourier_split)\n",
        "n_train_noisy = len(x_train_noisy_fourier_split)\n",
        "n_test_clean = len(x_test_clean_fourier_split)\n",
        "n_test_noisy = len(x_test_noisy_fourier_split)\n",
        "\n",
        "x_train_clean_fourier_norm = all_normalized[:n_train_clean]\n",
        "x_train_noisy_fourier_norm = all_normalized[n_train_clean:n_train_clean + n_train_noisy]\n",
        "x_test_clean_fourier_norm = all_normalized[n_train_clean + n_train_noisy:n_train_clean + n_train_noisy + n_test_clean]\n",
        "x_test_noisy_fourier_norm = all_normalized[-n_test_noisy:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UuBLkW25p3-"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JOiKQEwy5sKJ",
        "outputId": "c1ac5560-708e-461e-963a-d1654460f171"
      },
      "outputs": [],
      "source": [
        "print(\"Train Clean:\", x_train_clean_fourier_split.shape, x_train_clean_fourier_split.dtype)\n",
        "print(\"Train Noisy:\", x_train_noisy_fourier_split.shape)\n",
        "print(\"Test Clean:\", x_test_clean_fourier_split.shape)\n",
        "print(\"Test Noisy:\", x_test_noisy_fourier_split.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Channel Wise Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def describe_channels(data, label=\"\"):\n",
        "    mean = np.mean(data, axis=(0, 1, 2))\n",
        "    std = np.std(data, axis=(0, 1, 2))\n",
        "    print(f\"{label} - Mean per channel: {mean}\")\n",
        "    print(f\"{label} - Std  per channel: {std}\")\n",
        "\n",
        "describe_channels(x_train_clean_fourier_split, \"Train Clean\")\n",
        "describe_channels(x_train_noisy_fourier_split, \"Train Noisy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distribution and Value Ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histograms(data, title_prefix):\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(15, 6))\n",
        "    for i in range(6):\n",
        "        ax = axs[i // 3, i % 3]\n",
        "        ax.hist(data[..., i].ravel(), bins=100, color='steelblue', alpha=0.7)\n",
        "        ax.set_title(f\"{title_prefix} - Channel {i}\")\n",
        "        ax.set_xlim([-1, 1])  # Adjust depending on normalization\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_histograms(x_train_clean_fourier_split, \"Clean\")\n",
        "plot_histograms(x_train_noisy_fourier_split, \"Noisy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Spectral Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_spectral_image(image, title=\"\"):\n",
        "    # Expecting shape (H, W, 6)\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
        "    for i in range(6):\n",
        "        ax = axs[i // 3, i % 3]\n",
        "        ax.imshow(np.log1p(np.abs(image[..., i])), cmap='magma')\n",
        "        ax.set_title(f\"{title} - Channel {i}\")\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_spectral_image(x_train_clean_fourier_split[0], title=\"Clean\")\n",
        "show_spectral_image(x_train_noisy_fourier_split[0], title=\"Noisy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Difference Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diff = x_train_noisy_fourier_split[0] - x_train_clean_fourier_split[0]\n",
        "show_spectral_image(diff, title=\"Noisy - Clean Difference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGiURME3-qH"
      },
      "source": [
        "## Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 400\n",
        "batch_size = 16\n",
        "loss_function = \"mse\"\n",
        "model_name = f\"UDnCNN_fourier_{loss_function}_e{epochs}_b{batch_size}_v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4XiApan38tv",
        "outputId": "9ca2f68d-b55c-4d84-a8b6-d68e2df11b7f"
      },
      "outputs": [],
      "source": [
        "input_img = Input(shape=(x_train_clean_fourier_split.shape[1], x_train_clean_fourier_split.shape[2], x_train_clean_fourier_split.shape[3]))\n",
        "\n",
        "# Initial Conv + ReLU\n",
        "x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
        "x = ReLU()(x)\n",
        "\n",
        "# Downsampling block 1\n",
        "x1 = Conv2D(64, (3, 3), padding='same')(x)\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = ReLU()(x1)\n",
        "p1 = MaxPooling2D(pool_size=(2, 2))(x1)\n",
        "\n",
        "# Downsampling block 2\n",
        "x2 = Conv2D(64, (3, 3), padding='same')(p1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 = ReLU()(x2)\n",
        "p2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
        "\n",
        "# Middle block 1\n",
        "x3 = Conv2D(64, (3, 3), padding='same')(p2)\n",
        "x3 = BatchNormalization()(x3)\n",
        "x3 = ReLU()(x3)\n",
        "\n",
        "# Middle block 2\n",
        "x4 = Conv2D(64, (3, 3), padding='same')(x3)\n",
        "x4 = BatchNormalization()(x4)\n",
        "x4 = ReLU()(x4)\n",
        "\n",
        "# Upsampling block 1 + skip connection with x2\n",
        "u1 = UpSampling2D(size=(2, 2))(x4)\n",
        "u1 = Add()([u1, x2])\n",
        "u1 = Conv2D(64, (3, 3), padding='same')(u1)\n",
        "u1 = BatchNormalization()(u1)\n",
        "u1 = ReLU()(u1)\n",
        "\n",
        "# Upsampling block 2 + skip connection with x1\n",
        "u2 = UpSampling2D(size=(2, 2))(u1)\n",
        "u2 = Add()([u2, x1])\n",
        "u2 = Conv2D(64, (3, 3), padding='same')(u2)\n",
        "u2 = BatchNormalization()(u2)\n",
        "u2 = ReLU()(u2)\n",
        "\n",
        "# Final output layer\n",
        "output = Conv2D(x_train_clean_fourier_split.shape[3], (3, 3), padding='same')(u2)\n",
        "\n",
        "# Skip Connection (Residual Learning)\n",
        "output_img = Add()([input_img, output])\n",
        "\n",
        "model = Model(inputs=input_img, outputs=output_img)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss_function)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Callback to Track Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeHistory(Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.start_time = time.time()  # returns float in seconds\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.total_time = time.time() - self.start_time\n",
        "\n",
        "time_callback = TimeHistory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNlBUXuX5YCo",
        "outputId": "e322b4db-d9bd-4165-b1c7-756efa3bf3a2"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x_train_noisy_fourier_split, x_train_clean_fourier_split,\n",
        "    validation_data=(x_test_noisy_fourier_split, x_test_clean_fourier_split),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[time_callback]\n",
        ")\n",
        "print(\"Training time (seconds):\", round(time_callback.total_time, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dir = f\"./models/{model_name}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "model.save(os.path.join(model_dir, \"model.keras\"))\n",
        "print(f\"\\n✅ Model saved to: {model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_fourier_split = model.predict(x_test_noisy_fourier_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert Back to Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_fourier_split_denorm = denormalize_globally(predictions_fourier_split, global_mean, global_std)\n",
        "predictions_fourier = combine_complex_channels(predictions_fourier_split_denorm)\n",
        "predictions = [inverse_fourier_transform(f) for f in predictions_fourier]\n",
        "show_samples(x_test_clean, predictions, \"Clean\", \"Predicted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "mae = np.mean(np.abs(predictions - x_test_clean))\n",
        "mse = np.mean((predictions - x_test_clean) ** 2)\n",
        "\n",
        "psnr_total, ssim_total = 0, 0\n",
        "for i in range(len(x_test_clean)):\n",
        "    psnr_total += peak_signal_noise_ratio(x_test_clean[i], predictions[i], data_range=1.0)\n",
        "    ssim_total += structural_similarity(x_test_clean[i], predictions[i], channel_axis=-1, data_range=1.0)\n",
        "\n",
        "psnr_avg = psnr_total / len(x_test_clean)\n",
        "ssim_avg = ssim_total / len(x_test_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata = {\n",
        "    \"model_name\": model_name,\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"loss_function\": loss_function,\n",
        "    \"epochs\": int(epochs),\n",
        "    \"batch_size\": int(batch_size),\n",
        "    \"training_time_seconds\": round(float(time_callback.total_time), 2),\n",
        "    \"dataset_info\": {\n",
        "        \"train_size\": int(x_train_clean.shape[0]),\n",
        "        \"test_size\": int(x_test_clean.shape[0]),\n",
        "        \"image_shape\": list(map(int, x_train_clean.shape[1:]))\n",
        "    },\n",
        "    \"evaluation_metrics\": {\n",
        "        \"mae\": round(float(mae), 6),\n",
        "        \"mse\": round(float(mse), 6),\n",
        "        \"psnr_avg\": round(float(psnr_avg), 3),\n",
        "        \"ssim_avg\": round(float(ssim_avg), 3)\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(model_dir, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(f\"\\n✅ Metadata saved to: {model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz1_i-Qx696G"
      },
      "source": [
        "### Visualize the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_predictions(noisy, clean, predicted, num_images=5):\n",
        "    plt.figure(figsize=(num_images * 3, 9))\n",
        "    for i in range(num_images):\n",
        "        # Noisy\n",
        "        plt.subplot(3, num_images, i + 1)\n",
        "        plt.imshow(noisy[i])\n",
        "        plt.title(\"Noisy\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predicted\n",
        "        plt.subplot(3, num_images, i + 1 + num_images)\n",
        "        plt.imshow(predicted[i])\n",
        "        plt.title(\"Denoised\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Clean\n",
        "        plt.subplot(3, num_images, i + 1 + num_images * 2)\n",
        "        plt.imshow(clean[i])\n",
        "        plt.title(\"Clean\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_predictions(x_test_noisy, x_test_clean, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_and_save_denoising(noisy, predicted, clean, model_dir, num_images=5):\n",
        "    \"\"\"\n",
        "    Visualize and save a comparison of noisy, denoised, and clean images.\n",
        "    \n",
        "    Args:\n",
        "        noisy (numpy array): Noisy input images\n",
        "        predicted (numpy array): Model predictions (denoised images)\n",
        "        clean (numpy array): Ground truth clean images\n",
        "        model_dir (str): Directory where the visualization image will be saved\n",
        "        num_images (int): Number of image samples to visualize\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(num_images * 3, 9))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Noisy input\n",
        "        plt.subplot(3, num_images, i + 1)\n",
        "        plt.imshow(noisy[i])\n",
        "        plt.title(\"Noisy\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Denoised (model output)\n",
        "        plt.subplot(3, num_images, i + 1 + num_images)\n",
        "        plt.imshow(predicted[i])\n",
        "        plt.title(\"Denoised\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Ground truth\n",
        "        plt.subplot(3, num_images, i + 1 + 2 * num_images)\n",
        "        plt.imshow(clean[i])\n",
        "        plt.title(\"Clean\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save figure\n",
        "    save_path = os.path.join(model_dir, \"denoising_visualization.png\")\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()  # Close the figure to free memory\n",
        "\n",
        "    print(f\"📸 Visualization saved to: {save_path}\")\n",
        "\n",
        "visualize_and_save_denoising(\n",
        "    noisy=x_test_noisy,\n",
        "    predicted=predictions,\n",
        "    clean=x_test_clean,\n",
        "    model_dir=model_dir,\n",
        "    num_images=5  # You can increase this number if desired\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_image_and_fourier_grid(clean_imgs, noisy_imgs, denoised_imgs, num_samples=5):\n",
        "    def compute_fft_magnitude(img):\n",
        "        if len(img.shape) == 3 and img.shape[-1] == 3:\n",
        "            ffts = [np.fft.fftshift(np.fft.fft2(img[..., c])) for c in range(3)]\n",
        "            mags = [20 * np.log(np.abs(f) + 1) for f in ffts]\n",
        "            return np.mean(mags, axis=0)\n",
        "        else:\n",
        "            f = np.fft.fftshift(np.fft.fft2(img))\n",
        "            return 20 * np.log(np.abs(f) + 1)\n",
        "\n",
        "    def safe_imshow(img):\n",
        "        if img.ndim == 2:\n",
        "            plt.imshow(img, cmap='gray')\n",
        "        else:\n",
        "            if img.dtype in [np.float32, np.float64]:\n",
        "                img = np.clip(img, 0.0, 1.0)\n",
        "            else:\n",
        "                img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "            plt.imshow(img)\n",
        "\n",
        "    titles = [\"Clean\", \"Clean FT\", \"Noisy\", \"Noisy FT\", \"Denoised\", \"Denoised FT\"]\n",
        "    \n",
        "    plt.figure(figsize=(18, 3 * num_samples))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        clean = clean_imgs[i]\n",
        "        noisy = noisy_imgs[i]\n",
        "        denoised = denoised_imgs[i]\n",
        "        \n",
        "        imgs = [\n",
        "            clean,\n",
        "            compute_fft_magnitude(clean),\n",
        "            noisy,\n",
        "            compute_fft_magnitude(noisy),\n",
        "            denoised,\n",
        "            compute_fft_magnitude(denoised)\n",
        "        ]\n",
        "        \n",
        "        for j, img in enumerate(imgs):\n",
        "            plt.subplot(num_samples, 6, i * 6 + j + 1)\n",
        "            safe_imshow(img)\n",
        "            plt.axis('off')\n",
        "            if i == 0:\n",
        "                plt.title(titles[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_image_and_fourier_grid(\n",
        "    clean_imgs=x_test_clean,\n",
        "    noisy_imgs=x_test_noisy,\n",
        "    denoised_imgs=predictions,\n",
        "    num_samples=5\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep CNN Autoencoder - Denoising Image.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf310_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
